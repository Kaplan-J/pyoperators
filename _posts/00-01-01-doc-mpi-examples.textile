---
layout: post
title: 6. MPI examples
category: doc
---

h2. 6.1 <a name="distributeidentityoperator"></a> Using DistributeIdentityOperator

In this example, we will tell each MPI process to randomly and independly observe 20% of an input image and collect all this information to reconstruct the input image. Each observation is assumed to be affected by an independent gaussian noise.

{% highlight python %}
import numpy as np
import scipy
from mpi4py import MPI
from pyoperators import DistributionIdentityOperator, HomothetyOperator, MaskOperator, pcg

x0 = scipy.misc.lena().astype(float)
sigma = 2.
n = np.random.standard_normal(x0.shape) * sigma
invN = HomothetyOperator(1 / sigma**2)

H = MaskOperator(np.random.random(x0.shape) > 0.2) * \
    DistributionIdentityOperator()
y = H(x0) + n

A = H.T * invN * H
solution = pcg(A, H.T(invN(y)))

if MPI.COMM_WORLD.rank == 0:
    np.save('mpi_n{}.npy'.format(MPI.COMM_WORLD.size), solution['x'])
{% endhighlight %}

The @DistributeIdentityOperator@ is a block column operator whose blocks are the identity and for which the output of each block is handled by an MPI process according to its rank. Hence the MPI reduction is performed by the transpose of this operator (present in @H.T@): a block row operator whose blocks are the identity.
Results for 1, 2, 4 and 8 MPI processes are shown is the following figure.

<hr>
<img src="{{ site.baseurl}}/documentation/figures/lena_mpi1.png"> </img>

h2. 6.2 <a name="distributeglobaloperator"></a> Using DistributeGlobalOperator

TBD